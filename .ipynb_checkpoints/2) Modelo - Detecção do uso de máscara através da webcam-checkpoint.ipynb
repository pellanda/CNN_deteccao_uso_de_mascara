{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de termos rodado o notebook para preparar as imagens, fazemos as importações necessárias para darmos continuidade no projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trabalhar com o redimensionamento das imagens para 150px por 150px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Conv2D(32, 3, activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, 3, activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dropout(0.3), \n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(2, activation='softmax') # dense layer has a shape of 2 as we have only 2 classes \n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "acc = 0\n",
    "val_loss = 0\n",
    "val_acc = 0\n",
    "loss = 0\n",
    "val_loss = 0\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    training_dir = \"data/train\"\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                     rotation_range=40,\n",
    "                                     width_shift_range=0.2,\n",
    "                                     height_shift_range=0.2,\n",
    "                                     shear_range=0.2,\n",
    "                                     zoom_range=0.2,\n",
    "                                     horizontal_flip=True,\n",
    "                                     fill_mode='nearest')\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(training_dir, \n",
    "                                                      batch_size=10, \n",
    "                                                      target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    validation_dir = \"data/validation\"\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "    validation_generator = validation_datagen.flow_from_directory(validation_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=validation_generator,\n",
    "                                callbacks=[checkpoint])\n",
    "    global acc\n",
    "    acc = history.history['accuracy']\n",
    "    global val_acc\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    global loss\n",
    "    loss = history.history['loss']\n",
    "    global val_loss\n",
    "    val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    epochs_range = range(epochs)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing webcam to live preview face mask detection ROIs on faces\n",
    "# Seen this in this repo https://github.com/mk-gurucharan/Face-Mask-Detection\n",
    "face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0:'without mask',1:'with mask'}\n",
    "color_dict = {0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0)\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im = cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = im[y:(y+h)*size, x:(x+w) * size]\n",
    "        resized=cv2.resize(face_img,(IMG_WIDTH, IMG_HEIGHT))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized,(1,IMG_WIDTH,IMG_HEIGHT,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result = model.predict(reshaped)\n",
    "        \n",
    "        label = np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x* size,y* size),((x+w)* size,(y+h)* size),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x * size,(y* size)-40),((x+w)* size,y* size),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x* size + 10, (y* size)-10),cv2.FONT_HERSHEY_DUPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    cv2.imshow('Mask Detection', im)\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:\n",
    "        break\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model01.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
